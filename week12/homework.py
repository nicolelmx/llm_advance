"""
ğŸ“šç¬¬12å‘¨ä½œä¸šï¼šå†™ä¸€ä¸ªåŸºäºRagçš„æ–‡æ¡£ç¿»è¯‘ç³»ç»Ÿï¼Œå¯è‡ªç”±å‘æŒ¥

# ä½œä¸šå‚è€ƒèµ„æ–™

1. [RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
2. [TF-IDFç®—æ³•è¯¦è§£](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)
3. [ä½™å¼¦ç›¸ä¼¼åº¦](https://en.wikipedia.org/wiki/Cosine_similarity)
4. [æœºå™¨ç¿»è¯‘æŠ€æœ¯ç»¼è¿°](https://www.aclweb.org/anthology/2020.acl-main.1/)
"""
import math
import re
from typing import List, Tuple, Counter


class RAGTranslator:
    """RAGç¿»è¯‘å™¨ - åŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆçš„ç¿»è¯‘ç³»ç»Ÿ"""

    def __init__(self):
        self.vocabulary = {}
        self.stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ',
                           'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'the', 'a',
                           'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        self.documents = []
        self.vectors = []
        self.metadata = []
        self.translations = {}
    
    def preprocess_text(self, text: str) -> List[str]:
        """æ–‡æœ¬é¢„å¤„ç†"""
        # è½¬æ¢ä¸ºå°å†™ï¼Œå»é™¤æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        # åˆ†è¯
        words = text.split()
        # ç§»é™¤åœç”¨è¯
        words = [word for word in words if word not in self.stop_words and len(word) > 1]
        return words

    def get_documents_vectors(self, documents: List[str]) -> list:
        # è®¡ç®—æ–‡æ¡£å‘é‡
        vectors = []

        # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„è¯é¢‘
        doc_word_counts = []
        for doc in documents:
            words = self.preprocess_text(doc)
            word_count = Counter(words)
            doc_word_counts.append(word_count)

        # è®¡ç®—TF-IDF
        for word_count in doc_word_counts:
            vector = []
            for word in self.vocabulary:
                # TF: è¯é¢‘
                tf = word_count.get(word, 0) / len(word_count) if word_count else 0

                # IDF: é€†æ–‡æ¡£é¢‘ç‡
                doc_count = sum(1 for wc in doc_word_counts if word in wc)
                idf = math.log(len(doc_word_counts) / (doc_count + 1))

                # TF-IDF
                tf_idf = tf * idf
                vector.append(tf_idf)

            vectors.append(vector)

        return vectors

    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if len(vec1) != len(vec2):
            return 0.0

        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        magnitude1 = math.sqrt(sum(a * a for a in vec1))
        magnitude2 = math.sqrt(sum(b * b for b in vec2))

        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0

        return dot_product / (magnitude1 * magnitude2)

    def train(self, parallel_corpus: List[Tuple[str, str]]):
        source_docs = [pair[0] for pair in parallel_corpus]
        target_docs = [pair[1] for pair in parallel_corpus]
        self.documents = source_docs + target_docs

        word_count = Counter()
        for doc in self.documents:
            words = self.preprocess_text(doc)
            word_count.update(words)

        # æ„å»ºè¯æ±‡è¡¨ï¼Œè¿‡æ»¤ä½é¢‘è¯
        self.vocabulary = {word: idx for idx, (word, count) in enumerate(word_count.most_common()) if count >= 1}

        # è®¡ç®—æ–‡æ¡£å‘é‡
        vectors = self.get_documents_vectors(self.documents)
        self.vectors.extend(vectors)
        # é»˜è®¤å…ƒæ•°æ®
        for i, doc in enumerate(self.documents):
            self.metadata.append({
                'id': i,
                'language': 'chinese' if doc in source_docs else 'english',
                'length': len(doc),
                'word_count': len(doc.split())
            })
        # æ„å»ºç¿»è¯‘è®°å¿†åº“ æ·»åŠ ç¿»è¯‘å¯¹
        self.translations = {'chinese_english': {}, 'english_chinese': {}}
        for source_doc, target_doc in parallel_corpus:
            self.translations['chinese_english'][source_doc] = target_doc
            self.translations['english_chinese'][target_doc] = source_doc

    def translate(self, text: str, source_lang: str = 'chinese', target_lang: str = 'english') -> str:
        # 1. æ£€æŸ¥ç¿»è¯‘è®°å¿†åº“
        lang_key = f"{source_lang}_{target_lang}"
        if text in self.translations[lang_key]:
            return self.translations[lang_key][text]

        # 2. æŸ¥æ‰¾ç›¸ä¼¼ç¿»è¯‘
        text_words = set(text.lower().split())
        best_score = 0
        best_match = None
        for source_doc, target_doc in self.translations[lang_key].items():
            source_words = set(source_doc.lower().split())
            score = len(text_words.intersection(source_words))
            if score > best_score:
                best_score = score
                best_match = target_doc
        if best_match:
            return best_match

        # 3. ä½¿ç”¨RAGè¿›è¡Œç¿»è¯‘
        # è®¡ç®—æŸ¥è¯¢å‘é‡
        query_vector = self.get_documents_vectors([text])[0]

        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        if source_lang == 'chinese':
            # æ£€ç´¢ä¸­æ–‡æ–‡æ¡£
            similarities = []
            for i, doc_vector in enumerate(self.vectors):
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = self.cosine_similarity(query_vector, doc_vector)
                if self.metadata[i]['language'] == source_lang:
                    similarities.append((self.documents[i], similarity, self.metadata[i]))
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            similarities.sort(key=lambda x: x[1], reverse=True)
            relevant_docs = similarities[:3]

            # æ‰¾åˆ°å¯¹åº”çš„è‹±æ–‡ç¿»è¯‘
            for doc, similarity, metadata in relevant_docs:
                # æŸ¥æ‰¾å¯¹åº”çš„è‹±æ–‡ç¿»è¯‘
                if doc in self.translations.get(lang_key):
                    target = self.translations[lang_key][doc]
                    return f"[RAGç¿»è¯‘] {target}"

        return "[æ— åŒ¹é…ç¿»è¯‘] æœªæ‰¾åˆ°åˆé€‚çš„ç¿»è¯‘"


corpus = [
    ("äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯", "Artificial intelligence is a branch of computer science"),
    ("æœºå™¨å­¦ä¹ ä½¿è®¡ç®—æœºèƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ ", "Machine learning enables computers to learn automatically"),
    ("æ·±åº¦å­¦ä¹ åŸºäºç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ", "Deep learning is based on neural networks for learning"),
    ("è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶è®¡ç®—æœºç†è§£äººç±»è¯­è¨€",
     "Natural language processing studies how computers understand human language"),
    ("è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿç†è§£å›¾åƒ", "Computer vision enables machines to understand images"),
    ("æ•°æ®æŒ–æ˜ä»å¤§é‡æ•°æ®ä¸­å‘ç°æ¨¡å¼", "Data mining discovers patterns from large amounts of data"),
    ("æ¨èç³»ç»Ÿå¸®åŠ©ç”¨æˆ·æ‰¾åˆ°æ„Ÿå…´è¶£çš„å†…å®¹", "Recommendation systems help users find interesting content"),
    ("ç®—æ³•æ˜¯è§£å†³é—®é¢˜çš„æ­¥éª¤åºåˆ—", "Algorithms are sequences of steps to solve problems"),
    ("ç¼–ç¨‹è¯­è¨€æ˜¯äººä¸è®¡ç®—æœºäº¤æµçš„å·¥å…·", "Programming languages are tools for human-computer communication"),
    ("è½¯ä»¶å¼€å‘éœ€è¦ç³»ç»Ÿæ€§çš„æ–¹æ³•", "Software development requires systematic approaches")
]
test_texts = [
    "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",  # ç›´æ¥åŒ¹é…
    "æœºå™¨å­¦ä¹ ç®—æ³•å¾ˆå¤æ‚",  # ç›¸ä¼¼åŒ¹é…
    "åŒºå—é“¾æŠ€æœ¯å¾ˆæœ‰å‰æ™¯",  # RAGç¿»è¯‘
    "é‡å­è®¡ç®—æ˜¯æœªæ¥æŠ€æœ¯"  # æ— åŒ¹é…
]


if __name__ == '__main__':
    translator = RAGTranslator()
    translator.train(corpus)
    for text in test_texts:
        translation = translator.translate(text, 'chinese', 'english')
        print(f"åŸæ–‡: {text}")
        print(f"è¯‘æ–‡: {translation}")
