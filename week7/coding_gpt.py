import torch
from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Model
from torch.optim import AdamW
import numpy as np
import warnings
warnings.filterwarnings('ignore')

class GPTTeachingDemo:
    def __init__(self):
        """åˆå§‹åŒ–GPTæ•™å­¦æ¼”ç¤ºç±»"""
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

    def load_gpt_model(self):
        """1. åŠ è½½é¢„è®­ç»ƒGPTæ¨¡å‹"""
        print("=== 1. åŠ è½½GPTæ¨¡å‹ ===")

        try:
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained('gpt2')

            # è®¾ç½®pad token
            self.tokenizer.pad_token = self.tokenizer.eos_token

            # åŠ è½½GPTæ¨¡å‹
            self.gpt_model = GPT2LMHeadModel.from_pretrained('gpt2').to(self.device)
            self.gpt_base = GPT2Model.from_pretrained('gpt2').to(self.device)

            print("âœ“ GPTæ¨¡å‹åŠ è½½å®Œæˆ")
            print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.gpt_model.parameters()):,}")
            return True

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            print("ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ°æœ¬åœ°")
            print("   ç¦»çº¿æ¨¡å¼: è®¾ç½®ç¯å¢ƒå˜é‡ HF_HUB_OFFLINE=1")
            print("   æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°: ~/.cache/huggingface/hub/")
            return False

    def demonstrate_gpt_tokenization(self):
        """2. æ¼”ç¤ºGPTåˆ†è¯"""
        print("\n=== 2. GPTåˆ†è¯æ¼”ç¤º ===")

        text = "Natural language processing is revolutionizing AI"

        # åŸºæœ¬åˆ†è¯
        tokens = self.tokenizer.tokenize(text)
        print(f"åŸå§‹æ–‡æœ¬: {text}")
        print(f"åˆ†è¯ç»“æœ: {tokens}")

        # è½¬æ¢ä¸ºID
        input_ids = self.tokenizer.encode(text, add_special_tokens=True)
        print(f"è¾“å…¥ID: {input_ids}")
        print(f"ç‰¹æ®Štoken: [EOS]={self.tokenizer.eos_token_id}")

    def demonstrate_gpt_forward(self):
        """3. æ¼”ç¤ºGPTå‰å‘ä¼ æ’­"""
        print("\n=== 3. GPTå‰å‘ä¼ æ’­æ¼”ç¤º ===")

        text = "The future of AI is"
        inputs = self.tokenizer(text, return_tensors='pt').to(self.device)

        with torch.no_grad():
            outputs = self.gpt_model(**inputs, labels=inputs['input_ids'])

        print(f"è¾“å…¥æ–‡æœ¬: {text}")
        print(f"è¾“å‡ºlogitså½¢çŠ¶: {outputs.logits.shape}")
        print(f"æŸå¤±å€¼: {outputs.loss:.4f}")

    def demonstrate_text_generation(self):
        """4. æ¼”ç¤ºæ–‡æœ¬ç”Ÿæˆèƒ½åŠ›"""
        print("\n=== 4. GPTæ–‡æœ¬ç”Ÿæˆæ¼”ç¤º ===")

        # åŸºç¡€æ–‡æœ¬ç”Ÿæˆ
        prompt = "The benefits of artificial intelligence include"
        print(f"æç¤ºæ–‡æœ¬: {prompt}")

        inputs = self.tokenizer(prompt, return_tensors='pt').to(self.device)

        with torch.no_grad():
            # ç”Ÿæˆæ–‡æœ¬
            generated_outputs = self.gpt_model.generate(
                inputs['input_ids'],
                max_length=50,
                num_return_sequences=1,
                no_repeat_ngram_size=2,
                temperature=0.7,
                do_sample=True,
                top_k=50,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id
            )

        generated_text = self.tokenizer.decode(generated_outputs[0], skip_special_tokens=True)
        print(f"ç”Ÿæˆæ–‡æœ¬: {generated_text}")
        print()

        # æ¼”ç¤ºä¸åŒå‚æ•°çš„æ•ˆæœ
        print("=== ä¸åŒç”Ÿæˆå‚æ•°å¯¹æ¯” ===")

        # é«˜åˆ›é€ æ€§ç”Ÿæˆ (é«˜æ¸©åº¦)
        creative_outputs = self.gpt_model.generate(
            inputs['input_ids'],
            max_length=30,
            temperature=1.2,
            do_sample=True,
            top_k=50,
            pad_token_id=self.tokenizer.eos_token_id
        )
        creative_text = self.tokenizer.decode(creative_outputs[0], skip_special_tokens=True)
        print(f"é«˜åˆ›é€ æ€§: {creative_text}")

        # ä¿å®ˆç”Ÿæˆ (ä½æ¸©åº¦)
        conservative_outputs = self.gpt_model.generate(
            inputs['input_ids'],
            max_length=30,
            temperature=0.3,
            do_sample=True,
            top_k=50,
            pad_token_id=self.tokenizer.eos_token_id
        )
        conservative_text = self.tokenizer.decode(conservative_outputs[0], skip_special_tokens=True)
        print(f"ä¿å®ˆç”Ÿæˆ: {conservative_text}")

    def demonstrate_autoregressive_modeling(self):
        """5. æ¼”ç¤ºè‡ªå›å½’è¯­è¨€å»ºæ¨¡"""
        print("\n=== 5. è‡ªå›å½’è¯­è¨€å»ºæ¨¡æ¼”ç¤º ===")

        text = "The weather today is"
        print(f"è¾“å…¥åºåˆ—: {text}")

        inputs = self.tokenizer(text, return_tensors='pt').to(self.device)

        with torch.no_grad():
            outputs = self.gpt_model(**inputs)
            next_token_logits = outputs.logits[:, -1, :]

        # è·å–Top 5é¢„æµ‹
        top_k = 5
        top_tokens = torch.topk(next_token_logits, top_k, dim=-1)

        print("é¢„æµ‹ä¸‹ä¸€ä¸ªtoken (Top 5):")
        for i in range(top_k):
            token_id = top_tokens.indices[0, i].item()
            token = self.tokenizer.decode(token_id)
            prob = torch.softmax(next_token_logits, dim=-1)[0, token_id].item()
            print(f"  {token}: {prob:.4f}")

    def create_generation_dataset(self):
        """åˆ›å»ºæ–‡æœ¬ç”Ÿæˆæ•°æ®é›†"""
        # æ¨¡æ‹Ÿæ•…äº‹ç»­å†™æ•°æ®é›†
        prompts = [
            "Once upon a time, in a magical forest,",
            "The scientist discovered a new element that",
            "In the year 2050, artificial intelligence",
            "The young adventurer found an ancient map leading to"
        ]
        return prompts

    def fine_tune_gpt_generator(self):
        """6. GPTå¾®è°ƒå®æˆ˜ï¼šæ•…äº‹ç»­å†™"""
        print("\n=== 6. GPTå¾®è°ƒå®æˆ˜ï¼šæ•…äº‹ç»­å†™ ===")

        # å‡†å¤‡æ•°æ®
        prompts = self.create_generation_dataset()

        # è®­ç»ƒå‚æ•°
        optimizer = AdamW(self.gpt_model.parameters(), lr=5e-5)
        num_epochs = 2

        print("å¼€å§‹å¾®è°ƒè®­ç»ƒ...")

        for epoch in range(num_epochs):
            total_loss = 0

            for prompt in prompts:
                # ç¼–ç è¾“å…¥
                inputs = self.tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(self.device)

                # åˆ›å»ºæ ‡ç­¾ (è‡ªå›å½’ä»»åŠ¡)
                labels = inputs['input_ids'].clone()

                # å‰å‘ä¼ æ’­
                outputs = self.gpt_model(**inputs, labels=labels)
                loss = outputs.loss

                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            avg_loss = total_loss / len(prompts)
            print(f"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}")

        print("âœ“ å¾®è°ƒå®Œæˆ")

        # æµ‹è¯•å¾®è°ƒæ•ˆæœ
        test_prompt = "In a distant galaxy,"
        test_inputs = self.tokenizer(test_prompt, return_tensors='pt').to(self.device)

        with torch.no_grad():
            test_outputs = self.gpt_model.generate(
                test_inputs['input_ids'],
                max_length=40,
                temperature=0.8,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        generated_story = self.tokenizer.decode(test_outputs[0], skip_special_tokens=True)
        print(f"\nå¾®è°ƒåç”Ÿæˆæ•…äº‹:\n{generated_story}")

    def demonstrate_gpt_evolution(self):
        """7. GPTæŠ€æœ¯æ¼”è¿›æ¼”ç¤º"""
        print("\n=== 7. GPTæŠ€æœ¯æ¼”è¿› ===")

        evolution = {
            "GPT-1": {
                "å‚æ•°é‡": "1.17äº¿",
                "ç‰¹ç‚¹": "é¦–æ¬¡å¤§è§„æ¨¡é¢„è®­ç»ƒ",
                "æ€§èƒ½": "åŸºç¡€è¯­è¨€å»ºæ¨¡"
            },
            "GPT-2": {
                "å‚æ•°é‡": "15äº¿",
                "ç‰¹ç‚¹": "é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›",
                "æ€§èƒ½": "å¤šä»»åŠ¡é€‚åº”æ€§å¼º"
            },
            "GPT-3": {
                "å‚æ•°é‡": "1750äº¿",
                "ç‰¹ç‚¹": "å°‘æ ·æœ¬å’Œé›¶æ ·æœ¬å­¦ä¹ ",
                "æ€§èƒ½": "æ¥è¿‘äººç±»æ°´å¹³"
            },
            "GPT-4": {
                "å‚æ•°é‡": "æœªçŸ¥",
                "ç‰¹ç‚¹": "å¤šæ¨¡æ€èƒ½åŠ›",
                "æ€§èƒ½": "æ›´å¼ºçš„æ¨ç†èƒ½åŠ›"
            }
        }

        print("GPTç³»åˆ—æ¨¡å‹æ¼”è¿›:")
        for version, info in evolution.items():
            print(f"\n{version}:")
            for key, value in info.items():
                print(f"  {key}: {value}")

    def compare_gpt_vs_bert(self):
        """8. GPTä¸BERTå¯¹æ¯”åˆ†æ"""
        print("\n=== 8. GPT vs BERT å¯¹æ¯”åˆ†æ ===")

        comparison = {
            "æ¶æ„è®¾è®¡": {
                "GPT": "å•å‘Transformerè§£ç å™¨ï¼ˆè‡ªå›å½’ï¼‰",
                "BERT": "åŒå‘Transformerç¼–ç å™¨ï¼ˆè‡ªç¼–ç ï¼‰"
            },
            "æ³¨æ„åŠ›æœºåˆ¶": {
                "GPT": "æ©ç è‡ªæ³¨æ„åŠ›ï¼ˆå› æœæ©ç ï¼‰",
                "BERT": "å…¨è¯æ³¨æ„åŠ›ï¼ˆæ— æ©ç ï¼‰"
            },
            "é¢„è®­ç»ƒä»»åŠ¡": {
                "GPT": "è‡ªå›å½’è¯­è¨€å»ºæ¨¡ï¼ˆé¢„æµ‹ä¸‹ä¸€è¯ï¼‰",
                "BERT": "æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMLMï¼‰+ä¸‹ä¸€å¥é¢„æµ‹ï¼ˆNSPï¼‰"
            },
            "æ ¸å¿ƒä¼˜åŠ¿": {
                "GPT": "æ–‡æœ¬ç”Ÿæˆã€åˆ›æ„å†™ä½œã€å¯¹è¯ç³»ç»Ÿ",
                "BERT": "æ–‡æœ¬ç†è§£ã€åˆ†ç±»ã€é—®ç­”ç³»ç»Ÿ"
            },
            "åº”ç”¨åœºæ™¯": {
                "GPT": "å†…å®¹åˆ›ä½œï¼ˆé‡‡çº³ç‡72%ï¼‰ã€ä»£ç ç”Ÿæˆ",
                "BERT": "æ–‡æœ¬åˆ†ç±»ï¼ˆå‡†ç¡®ç‡94.9%ï¼‰ã€NERï¼ˆF1å€¼96.6%ï¼‰"
            },
            "å‚æ•°è§„æ¨¡": {
                "GPT": "GPT-3è¾¾1750äº¿å‚æ•°",
                "BERT": "Baseç‰ˆ1.1äº¿å‚æ•°"
            }
        }

        print("GPTä¸BERTæ ¸å¿ƒå¯¹æ¯”:")
        for aspect, models in comparison.items():
            print(f"\n{aspect}:")
            print(f"  GPT: {models['GPT']}")
            print(f"  BERT: {models['BERT']}")

    def demonstrate_generation_strategies(self):
        """9. æ–‡æœ¬ç”Ÿæˆç­–ç•¥æ¼”ç¤º"""
        print("\n=== 9. æ–‡æœ¬ç”Ÿæˆç­–ç•¥æ¼”ç¤º ===")

        prompt = "Machine learning is"

        strategies = {
            "è´ªå©ªè§£ç ": {"do_sample": False, "temperature": 1.0},
            "éšæœºé‡‡æ ·": {"do_sample": True, "temperature": 1.0, "top_k": 50},
            "Top-pé‡‡æ ·": {"do_sample": True, "temperature": 0.8, "top_p": 0.9},
            "Top-ké‡‡æ ·": {"do_sample": True, "temperature": 0.8, "top_k": 40}
        }

        inputs = self.tokenizer(prompt, return_tensors='pt').to(self.device)

        print(f"æç¤º: {prompt}")
        print("\nä¸åŒç”Ÿæˆç­–ç•¥ç»“æœ:")

        for strategy_name, params in strategies.items():
            with torch.no_grad():
                outputs = self.gpt_model.generate(
                    inputs['input_ids'],
                    max_length=20,
                    pad_token_id=self.tokenizer.eos_token_id,
                    **params
                )

            generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"\n{strategy_name}: {generated[len(prompt):].strip()}")

    def run_all_demos(self):
        """è¿è¡Œæ‰€æœ‰GPTæ•™å­¦æ¼”ç¤º"""
        print("ğŸš€ å¼€å§‹GPTæ•™å­¦æ¼”ç¤º\n")

        if not self.load_gpt_model():
            print("\nâš ï¸  ç”±äºæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡éœ€è¦æ¨¡å‹çš„æ¼”ç¤º")
            print("ğŸ“– æ‚¨å¯ä»¥æŸ¥çœ‹ä»£ç æ³¨é‡Šäº†è§£GPTçš„å·¥ä½œåŸç†")
            return

        self.demonstrate_gpt_tokenization()
        self.demonstrate_gpt_forward()
        self.demonstrate_text_generation()
        self.demonstrate_autoregressive_modeling()
        self.fine_tune_gpt_generator()
        self.demonstrate_gpt_evolution()
        self.compare_gpt_vs_bert()
        self.demonstrate_generation_strategies()

        print("\nğŸ‰ GPTæ•™å­¦æ¼”ç¤ºå®Œæˆï¼")

# ä¸»ç¨‹åº
if __name__ == "__main__":
    demo = GPTTeachingDemo()
    demo.run_all_demos()