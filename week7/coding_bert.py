import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification
from torch.optim import AdamW
import numpy as np
from sklearn.metrics import accuracy_score, f1_score
import warnings
warnings.filterwarnings('ignore')

class BERTTeachingDemo:
    def __init__(self):
        """åˆå§‹åŒ–BERTæ•™å­¦æ¼”ç¤ºç±»"""
        # self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

    def load_bert_model(self):
        """1. åŠ è½½é¢„è®­ç»ƒBERTæ¨¡å‹"""
        print("=== 1. åŠ è½½BERTæ¨¡å‹ ===")

        try:
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')

            # åŠ è½½BERTæ¨¡å‹
            self.bert_model = BertModel.from_pretrained('bert-base-chinese').to(self.device)

            # åŠ è½½MLMä»»åŠ¡æ¨¡å‹
            self.mlm_model = BertForMaskedLM.from_pretrained('bert-base-chinese').to(self.device)

            print("âœ“ BERTæ¨¡å‹åŠ è½½å®Œæˆ")
            print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.bert_model.parameters()):,}")
            return True

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            print("ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ°æœ¬åœ°")
            print("   ç¦»çº¿æ¨¡å¼: è®¾ç½®ç¯å¢ƒå˜é‡ HF_HUB_OFFLINE=1")
            print("   æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°: ~/.cache/huggingface/hub/")
            return False

    def demonstrate_tokenization(self):
        """2. æ¼”ç¤ºBERTåˆ†è¯"""
        print("\n=== 2. BERTåˆ†è¯æ¼”ç¤º ===")

        text = "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•"

        # åŸºæœ¬åˆ†è¯
        tokens = self.tokenizer.tokenize(text)
        print(f"åŸå§‹æ–‡æœ¬: {text}")
        print(f"åˆ†è¯ç»“æœ: {tokens}")

        # è½¬æ¢ä¸ºID add_special_tokensé»˜è®¤trueï¼Œåˆ†è¯ç»“æœä¼šåŒ…å«ç‰¹æ®Šæ ‡è®°ã€‚[CLS]ï¼šå¥å­çš„èµ·å§‹æ ‡è®°ï¼ˆä½äºå¥é¦–ï¼‰[SEP]ï¼šå¥å­çš„åˆ†éš”æ ‡è®°
        input_ids = self.tokenizer.encode(text, add_special_tokens=True)
        print(f"è¾“å…¥ID: {input_ids}")
        print(f"ç‰¹æ®Štoken: [CLS]={self.tokenizer.cls_token_id}, [SEP]={self.tokenizer.sep_token_id}")

    def demonstrate_attention_mask(self):
        """3. æ¼”ç¤ºæ³¨æ„åŠ›æ©ç """
        print("\n=== 3. æ³¨æ„åŠ›æ©ç æ¼”ç¤º ===")

        texts = ["ä»Šå¤©å¤©æ°”å¾ˆå¥½", "è‡ªç„¶è¯­è¨€å¤„ç†å¾ˆæœ‰è¶£"]
        encoded = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')

        print("æ‰¹é‡ç¼–ç ç»“æœ:")
        print(f"è¾“å…¥IDå½¢çŠ¶: {encoded['input_ids'].shape}")
        print(f"æ³¨æ„åŠ›æ©ç : {encoded['attention_mask']}")

    def demonstrate_bert_forward(self):
        """4. æ¼”ç¤ºBERTå‰å‘ä¼ æ’­"""
        print("\n=== 4. BERTå‰å‘ä¼ æ’­æ¼”ç¤º ===")

        text = "BERTæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä¸°å¯Œçš„è¯­è¨€è¡¨å¾"
        inputs = self.tokenizer(text, return_tensors='pt').to(self.device)

        with torch.no_grad():
            outputs = self.bert_model(**inputs)

        print(f"è¾“å…¥æ–‡æœ¬: {text}")
        print(f"last_hidden_stateå½¢çŠ¶: {outputs.last_hidden_state.shape}")
        print(f"pooler_outputå½¢çŠ¶: {outputs.pooler_output.shape}")

        # [CLS] tokençš„è¡¨å¾
        cls_embedding = outputs.last_hidden_state[0, 0, :]
        print(f"[CLS] tokenè¡¨å¾ç»´åº¦: {cls_embedding.shape}")

    def demonstrate_mlm_task(self):
        """5. æ¼”ç¤ºæ©ç è¯­è¨€æ¨¡å‹ä»»åŠ¡"""
        print("\n=== 5. æ©ç è¯­è¨€æ¨¡å‹(MLM)æ¼”ç¤º ===")

        text = "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦ç ”ç©¶æ–¹å‘ã€‚"
        print(f"åŸå§‹æ–‡æœ¬: {text}")

        # ä½¿ç”¨tokenizerè¿›è¡Œåˆ†è¯ï¼Œç„¶ååˆ›å»ºæ©ç 
        tokens = self.tokenizer.tokenize(text)
        print(f"åˆ†è¯ç»“æœ: {tokens}")
        
        # åˆ›å»ºæ©ç è¾“å…¥
        masked_tokens = tokens.copy()
        mask_positions = []

        # éšæœºæ©ç 15%çš„tokenï¼ˆè·³è¿‡ç‰¹æ®Štokenï¼‰
        for i, token in enumerate(tokens):
            if token not in ['[CLS]', '[SEP]'] and np.random.random() < 0.15:
                masked_tokens[i] = "[MASK]"
                mask_positions.append(i)

        # é‡æ–°ç¼–ç ä¸ºå®Œæ•´å¥å­
        masked_sentence = self.tokenizer.convert_tokens_to_string(masked_tokens)
        print(f"æ©ç å: {masked_sentence}")
        print(f"æ©ç ä½ç½®: {mask_positions}")

        # æ¨¡å‹é¢„æµ‹
        inputs = self.tokenizer(masked_sentence, return_tensors='pt').to(self.device)

        with torch.no_grad():
            outputs = self.mlm_model(**inputs)
            predictions = outputs.logits

        # è·å–é¢„æµ‹ç»“æœ
        mask_token_index = torch.where(inputs['input_ids'] == self.tokenizer.mask_token_id)[1]
        print('mask_token_index', mask_token_index)

        if len(mask_token_index) > 0:
            for i, pos in enumerate(mask_token_index):
                predicted_token_id = predictions[0, pos].argmax().item()
                predicted_token = self.tokenizer.decode(predicted_token_id)
                print(f"ä½ç½®{pos.item()}: [MASK] -> {predicted_token}")
        else:
            print("æœ¬æ¬¡è¿è¡Œæ²¡æœ‰ç”Ÿæˆæ©ç tokenï¼Œè¿™æ˜¯æ­£å¸¸çš„éšæœºè¡Œä¸º")

    def create_classification_dataset(self):
        """åˆ›å»ºæ–‡æœ¬åˆ†ç±»æ•°æ®é›†"""
        # æ¨¡æ‹Ÿæ–°é—»åˆ†ç±»æ•°æ®é›†
        texts = [
            "ç§‘æŠ€å…¬å¸å‘å¸ƒæ–°æ¬¾æ™ºèƒ½æ‰‹æœº",  # ç§‘æŠ€
            "æ”¿åºœå‡ºå°æ–°çš„ç»æµæ”¿ç­–",      # æ”¿æ²»
            "è¶³çƒæ¯”èµ›ç²¾å½©çº·å‘ˆ",          # ä½“è‚²
            "ç”µå½±ç¥¨æˆ¿åˆ›ä¸‹æ–°é«˜",          # å¨±ä¹
            "è‚¡å¸‚å‡ºç°å¤§å¹…æ³¢åŠ¨",          # è´¢ç»
        ]
        labels = [0, 1, 2, 3, 4]  # å¯¹åº”çš„æ ‡ç­¾

        return texts, labels

    def fine_tune_bert_classifier(self):
        """6. BERTå¾®è°ƒå®æˆ˜ï¼šæ–‡æœ¬åˆ†ç±»"""
        print("\n=== 6. BERTå¾®è°ƒå®æˆ˜ï¼šæ–°é—»åˆ†ç±» ===")

        # å‡†å¤‡æ•°æ®
        texts, labels = self.create_classification_dataset()
        labels = torch.tensor(labels).to(self.device)

        # åŠ è½½åˆ†ç±»æ¨¡å‹
        classifier = BertForSequenceClassification.from_pretrained(
            'bert-base-chinese',
            num_labels=5
        ).to(self.device)

        # ç¼–ç æ•°æ®
        inputs = self.tokenizer(texts, padding=True, truncation=True,
                               return_tensors='pt').to(self.device)

        # è®­ç»ƒå‚æ•°
        optimizer = AdamW(classifier.parameters(), lr=2e-5)
        num_epochs = 3
        batch_size = 2

        print("å¼€å§‹å¾®è°ƒè®­ç»ƒ...")

        for epoch in range(num_epochs):
            classifier.train()

            # å‰å‘ä¼ æ’­
            outputs = classifier(**inputs, labels=labels)
            loss = outputs.loss
            logits = outputs.logits

            # probabilities = F.softmax(logits, dim=1)  # è½¬æ¢ä¸ºæ¦‚ç‡
            # predictions = torch.argmax(probabilities, dim=1)  # é¢„æµ‹æ ‡ç­¾ç´¢å¼•
            # print(predictions)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # è®¡ç®—å‡†ç¡®ç‡
            predictions = torch.argmax(logits, dim=1)
            print(predictions)
            accuracy = (predictions == labels).float().mean()
            print(predictions)
            print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.3f}, Accuracy: {accuracy.item():.3f}")

        print("âœ“ å¾®è°ƒå®Œæˆ")

    def compare_bert_vs_gpt(self):
        """7. BERTä¸GPTå¯¹æ¯”åˆ†æ"""
        print("\n=== 7. BERT vs GPT å¯¹æ¯”åˆ†æ ===")

        comparison = {
            "æ¶æ„": {
                "BERT": "åŒå‘Transformerç¼–ç å™¨",
                "GPT": "å•å‘Transformerè§£ç å™¨"
            },
            "æ³¨æ„åŠ›æœºåˆ¶": {
                "BERT": "å…¨è¯å…³æ³¨ï¼ˆæ— æ©ç ï¼‰",
                "GPT": "æ©ç è‡ªæ³¨æ„åŠ›"
            },
            "ä¼˜åŠ¿åœºæ™¯": {
                "BERT": "æ–‡æœ¬ç†è§£ä»»åŠ¡",
                "GPT": "æ–‡æœ¬ç”Ÿæˆä»»åŠ¡"
            },
            "é¢„è®­ç»ƒä»»åŠ¡": {
                "BERT": "MLM + NSP",
                "GPT": "è‡ªå›å½’è¯­è¨€å»ºæ¨¡"
            },
            "å‚æ•°é‡çº§": {
                "BERT": "Baseç‰ˆ1.1äº¿å‚æ•°",
                "GPT": "GPT-3è¾¾1750äº¿å‚æ•°"
            }
        }

        print("BERTä¸GPTæ ¸å¿ƒå¯¹æ¯”:")
        for aspect, models in comparison.items():
            print(f"{aspect}:")
            print(f"  BERT: {models['BERT']}")
            print(f"  GPT: {models['GPT']}")
            print()

    def run_all_demos(self):
        """è¿è¡Œæ‰€æœ‰BERTæ•™å­¦æ¼”ç¤º"""
        print("ğŸš€ å¼€å§‹BERTæ•™å­¦æ¼”ç¤º\n")

        if not self.load_bert_model():
            print("\nâš ï¸  ç”±äºæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡éœ€è¦æ¨¡å‹çš„æ¼”ç¤º")
            print("ğŸ“– æ‚¨å¯ä»¥æŸ¥çœ‹ä»£ç æ³¨é‡Šäº†è§£BERTçš„å·¥ä½œåŸç†")
            return

        self.demonstrate_tokenization()
        self.demonstrate_attention_mask()
        self.demonstrate_bert_forward()
        self.demonstrate_mlm_task()
        self.fine_tune_bert_classifier()
        # self.compare_bert_vs_gpt()

        print("\nğŸ‰ BERTæ•™å­¦æ¼”ç¤ºå®Œæˆï¼")

# ä¸»ç¨‹åº
if __name__ == "__main__":
    demo = BERTTeachingDemo()
    demo.run_all_demos()
